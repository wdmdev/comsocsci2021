{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Building the GME redditors network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, enough with theory :) It is time to go back to our cool dataset it took us so much pain to download! And guess what? We will build the network of GME Redditors. Then, we will use some Network Science to study some of its properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> \n",
    "> *Exercise*: Build the network of Redditors discussing about GME on r\\wallstreetbets. In this network, nodes correspond to authors of comments, and a direct link going from node _A_ to node _B_ exists if _A_ ever answered a submission or a comment by _B_. The weight on the link corresponds to the number of times _A_ answered _B_. You can build the network as follows:\n",
    ">\n",
    "> 1. Open the _comments dataset_ and the _submission datasets_ (the first contains all the comments and the second cointains all the submissions) and store them in two Pandas DataFrames.\n",
    "> 2. Create three dictionaries, using the command ``dict(zip(keys,values))``, where keys and values are columns in your dataframes. The three dictionaries are the following:\n",
    ">     * __comment_authors__: (_comment id_, _comment author_)\n",
    ">     * __parent__:  (_comment id_ , _parent id_)\n",
    ">     * __submission_authors__: (_submission id_, _submission author_)\n",
    ">\n",
    "> where above I indicated the (key, value) tuples contained in each dictionary.\n",
    ">\n",
    "> 3. Create a function that take as input a _comment id_ and outputs the author of its parent. The function does two things:\n",
    ">     * First, it calls the dictionary __parent__, to find the _parent id_ of the comment identified by a given _comment id_. \n",
    ">     * Then, it finds the author of  _parent id_. \n",
    ">          * if the _parent id_ starts with \"t1_\", call the __comment_authors__ dictionary (for key=parent_id[3:])\n",
    ">          * if the _parent id_ starts with \"t3_\", call the __submission_authors__ dictionars (for key=parent_id[3:])\n",
    ">\n",
    "> where by parent_id[3:], I mean that the first three charachters of the _parent id_ (either \"t1_\" or \"t3_\" should be ingnored).\n",
    ">\n",
    "> 4. Apply the function you created in step 3. to all the comment ids in your comments dataframe. Store the output in a new column, _\"parent author\"_, of the comments dataframe. \n",
    "> 5. For now, we will focus on the genesis of the GME community on Reddit, before all the hype started and many new redditors jumped on board. For this reason, __filter all the comments written before Dec 31st, 2020__. Also, remove deleted users by filtering all comments whose author or parent author is equal to \"[deleted]\". \n",
    "> 6. Create the weighted edge-list of your network as follows: consider all comments (after applying the filtering step above), groupby (\"_author_\", _\"parent author\"_) and count. \n",
    "> 7. Create a [``DiGraph``](https://networkx.org/documentation/stable//reference/classes/digraph.html) using networkx. Then, use the networkx function [``add_weighted_edges_from``](https://networkx.org/documentation/networkx-1.9/reference/generated/networkx.DiGraph.add_weighted_edges_from.html) to create a weighted, directed, graph starting from the edgelist you created in step 5."
   ]
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "source": [
    "### 1) Dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(os.path.join('Data', 'wallstreetbets_submissions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def dateparse (time_in_secs):    \n",
    "    return datetime.datetime.fromtimestamp(float(time_in_secs))\n",
    "\n",
    "comments = pd.read_csv(os.path.join('Data', 'wallstreetbets_comments.csv'), parse_dates=['created'], date_parser=dateparse)"
   ]
  },
  {
   "source": [
    "### 2) Dictionaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_authors = dict(zip(comments.id, comments.author))\n",
    "parent = dict(zip(comments.id, comments.parent_id))\n",
    "submission_authors = dict(zip(submissions.id, submissions.author))"
   ]
  },
  {
   "source": [
    "### 3) Comment id to Parent Author Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_to_parent_author(comment_id):\n",
    "    try:\n",
    "        pid = parent[comment_id]\n",
    "        if pid.startswith('t1_'):\n",
    "            return comment_authors[pid[3:]] #ignore the first 3 character\n",
    "        elif pid.startswith('t3_'):\n",
    "            return submission_authors[pid[3:]]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "source": [
    "### 4) Parent author Comments Dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments['parent_author'] = comments.id.apply(comment_to_parent_author)"
   ]
  },
  {
   "source": [
    "### 5) Filter comments, with authors and before hype"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "85958"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import datetime as dt\n",
    "filtered_comments = comments[comments.created < dt.datetime.strptime('31 December, 2020', '%d %B, %Y')]\n",
    "filtered_comments = filtered_comments[filtered_comments.author is not None and filtered_comments.author != 'deleted']\n",
    "filtered_comments = filtered_comments[filtered_comments.parent_author is not None and filtered_comments.parent_author != 'deleted']\n",
    "len(filtered_comments)"
   ]
  },
  {
   "source": [
    "### 6) Weighted edge list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edge_list = filtered_comments.groupby(['author', 'parent_author']).count()"
   ]
  },
  {
   "source": [
    "### 7) Directed graph "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(filtered_comments.author)\n",
    "G.add_edges_from([(a,pa, {'weight': w}) for a,pa,w in zip(filtered_comments.author, filtered_comments.parent_author, filtered_comments.id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Preliminary analysis of the GME redditors network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a preliminary analysis of the network.\n",
    "\n",
    "> \n",
    "> *Exercise: Basic Analysis of the Redditors Network*\n",
    "> * Why do you think I want you guys to use a _directed_ graph? Could have we used an undirected graph instead?\n",
    "> * What is the total number of nodes in the network? What is the total number of links? What is the density of the network (the total number of links over the maximum number of links)?\n",
    "> * What are the average, median, mode, minimum and maximum value of the in-degree (number of incoming edges per redditor)? And of the out-degree (number of outgoing edges per redditor)? How do you intepret the results?\n",
    "> * List the top 5 Redditors by in-degree and out-degree. What is their average score over time? At which point in time did they join the discussion on GME? When did they leave it?\n",
    "> * Plot the distribution of in-degrees and out-degrees, using a logarithmic binning (see last week's exercise 4). \n",
    "> * Plot a scatter plot of the the in- versus out- degree for all redditors. Comment on the relation between the two.\n",
    "> * Plot a scatter plot of the the in- degree versus average score for all redditors. Comment on the relation between the two.\n"
   ]
  },
  {
   "source": [
    "### Bullet Point 1)\n",
    "I think, you want us to see the child parent relation ship. For instance it could show that some people are \"first movers\" in the comments and other are \"followers\", who mainly comment on the \"first movers\" postings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bullet Point 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24198"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# Total number of nondes\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "67480"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# Totale number of links\n",
    "len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network density: 0.00023049636069371248\n"
     ]
    }
   ],
   "source": [
    "n = len(G.nodes)\n",
    "max_link_count = n*(n-1)/2\n",
    "actual_link_count = len(G.edges)\n",
    "density = actual_link_count/max_link_count\n",
    "print(f'Network density: {density}')"
   ]
  },
  {
   "source": [
    "### Bullet Point 3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "in_degrees = list(dict(G.in_degree).values())\n",
    "out_degrees = list(dict(G.out_degree).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def degree_calc(deg_type:str, degrees):\n",
    "    print(f'Average {deg_type}: {np.mean(degrees):.3}')\n",
    "    print(f'Median {deg_type}: {np.median(degrees):.3}')\n",
    "    print(f'{deg_type} mode: {stats.mode(degrees).mode[0]}')\n",
    "    print(f'Min {deg_type}: {min(degrees)}')\n",
    "    print(f'Max {deg_type}: {max(degrees)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average in-degree: 2.79\nMedian in-degree: 0.0\nin-degree mode: 0\nMin in-degree: 0\nMax in-degree: 5322\n"
     ]
    }
   ],
   "source": [
    "degree_calc('in-degree', in_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average out-degree: 2.79\nMedian out-degree: 1.0\nout-degree mode: 1\nMin out-degree: 0\nMax out-degree: 1496\n"
     ]
    }
   ],
   "source": [
    "degree_calc('out-degree', out_degrees)"
   ]
  },
  {
   "source": [
    "* From the median an mode it seems like it is more typical that people do not get comments on what they post (in-degree median and mode is 0). While the out-degree median and mode is 1. This could mean that mostly people make a single comment and they do it on content created by few people.\n",
    "\n",
    "* Also we can see that at least one author gets a lot of comments and at least one author is very active commenting as the max in/out degrees are thousand of degrees higher than the min, mode, median, and average"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bullet Point 4)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Bullet Point 5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Bullet Point 6)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Bullet Point 7)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}